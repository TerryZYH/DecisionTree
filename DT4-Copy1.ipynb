{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict \n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from copy import deepcopy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset,n,i,seed=0):\n",
    "    \"\"\"\n",
    "    A functon to split a given dataset into train dataset and test dataset following n cross validation. The dataset\n",
    "    is splited into n subsets. The i th subset is choosen to be the test dataset and the rest are combined to be the\n",
    "    training dataset.\n",
    "    \n",
    "    INPUTS:\n",
    "        dataset: A ndarray of shape (# of observations * sample dimensions). \n",
    "        n: A parameter used for n cross validation.\n",
    "        i: A parameter used for n cross validation.\n",
    "    \n",
    "    OUTPUTs:\n",
    "        train_dataset\n",
    "        test_dataset\n",
    "    \"\"\"\n",
    "    datasize = len(dataset)\n",
    "    train_dataset = np.vstack((dataset[0:int(i*datasize/n)],dataset[int((i+1)*datasize/n):]))\n",
    "    test_dataset = dataset[int(i*datasize/n):int((i+1)*datasize/n)]\n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,attribute=None,value=None,left=None,right=None,left_dataset=None,right_dataset=None,is_a_leaf=False,leaf_value=None,depth=None,parent=None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.left_dataset = left_dataset\n",
    "        self.right_dataset = right_dataset\n",
    "        self.is_a_leaf = is_a_leaf\n",
    "        self.leaf_value = leaf_value\n",
    "        self.depth = depth\n",
    "        self.position = None\n",
    "        self.p_position = None\n",
    "        self.parent = parent\n",
    "        self.keep = False\n",
    "        self.major_value = None\n",
    "\n",
    "def get_major_value(dataset):\n",
    "    count = defaultdict(lambda: 0)\n",
    "    max_count=0\n",
    "    max_label=None\n",
    "    for data in dataset:\n",
    "        count[data[-1]ata[-1]]+=1\n",
    "    for label in count.keys():\n",
    "        if count[label]>max_count:\n",
    "            max_count = count[label]\n",
    "            max_label = label\n",
    "    return label\n",
    "        \n",
    "def decision_tree_learning(training_dataset,depth):\n",
    "    if is_leaf(training_dataset):\n",
    "        leaf_value = training_dataset[0,-1]\n",
    "        return Node(is_a_leaf=True,leaf_value=leaf_value,depth=depth),depth\n",
    "    else:\n",
    "        major_value = get_major_value(training_dataset)\n",
    "        \n",
    "        \n",
    "        attribute,value,IG = find_split(training_dataset)\n",
    "        \n",
    "        left_dataset_filter = training_dataset[:,attribute] <= value\n",
    "        right_dataset_filter = training_dataset[:,attribute] > value\n",
    "        left_dataset = training_dataset[left_dataset_filter]\n",
    "        right_dataset = training_dataset[right_dataset_filter]\n",
    "        \n",
    "        left_branch,left_depth = decision_tree_learning(left_dataset,depth+1)\n",
    "        right_branch,right_depth = decision_tree_learning(right_dataset,depth+1)\n",
    "        this_node = Node(attribute,value,left_branch,right_branch,left_dataset,right_dataset,depth=depth)\n",
    "        this_node.major_value = major_value\n",
    "        left_branch.parent = this_node\n",
    "        right_branch.parent = this_node\n",
    "        return this_node,max(left_depth,right_depth)\n",
    "    \n",
    "def is_leaf(training_dataset):\n",
    "    for label in training_dataset[1:,-1]:\n",
    "        if label != training_dataset[0,-1]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_split(training_dataset):\n",
    "    entropy = calc_entropy(training_dataset)\n",
    "    dataset_size = training_dataset.shape[0]\n",
    "    split_attribute = None\n",
    "    split_value = None\n",
    "    max_IG = 0\n",
    "    for attribute in range(training_dataset.shape[1]-1):\n",
    "        ordered_values = sorted(list(set(training_dataset[:,attribute])))\n",
    "        for value in ordered_values[:-1]:\n",
    "            left_dataset,right_dataset = filter_dataset(training_dataset,attribute,value)\n",
    "            left_dataset_size, right_dataset_size = left_dataset.shape[0], right_dataset.shape[0]\n",
    "            information_gain = entropy - left_dataset_size/dataset_size * calc_entropy(left_dataset) - right_dataset_size/dataset_size * calc_entropy(right_dataset)\n",
    "            if information_gain > max_IG:\n",
    "                max_IG = information_gain\n",
    "                split_value = value\n",
    "                split_attribute = attribute\n",
    "    return split_attribute,split_value,max_IG\n",
    "\n",
    "def filter_dataset(training_dataset,attribute,value):\n",
    "    left_dataset_filter = training_dataset[:,attribute] <= value\n",
    "    right_dataset_filter = training_dataset[:,attribute] > value\n",
    "    left_dataset = training_dataset[left_dataset_filter]\n",
    "    right_dataset = training_dataset[right_dataset_filter]\n",
    "    return left_dataset,right_dataset\n",
    "\n",
    "def calc_entropy(training_dataset):\n",
    "    label_count = defaultdict(lambda: 0)\n",
    "    dataset_size = training_dataset.shape[0]\n",
    "    entropy = 0\n",
    "    for label in training_dataset[:,-1]:\n",
    "        label_count[label] += 1\n",
    "    for value in label_count.values():\n",
    "        ratio = value/dataset_size\n",
    "        entropy -= ratio*np.log2(ratio)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def count_node(node,count_dict):\n",
    "    count_dict[node.depth]+=1\n",
    "    if not node.is_a_leaf:\n",
    "        count_node(node.left,count_dict)\n",
    "        count_node(node.right,count_dict)\n",
    "    return\n",
    "\n",
    "def assign_position(node,count,track,max_width,max_depth):\n",
    "    track[node.depth]+=1\n",
    "    x = max_width*3.5/(count[node.depth]+1)*track[node.depth]-2\n",
    "    y = node.depth*6+1.5\n",
    "    node.position=(x,y)\n",
    "    if node.depth == 0:\n",
    "        node.p_position = (x,y)\n",
    "    if not node.is_a_leaf:\n",
    "        node.left.p_position=(x,y-0.05)\n",
    "        node.right.p_position=(x,y-0.05)\n",
    "        assign_position(node.left,count,track,max_width,max_depth)\n",
    "        assign_position(node.right,count,track,max_width,max_depth)\n",
    "    return\n",
    "\n",
    "def add_boxes(nodeboxes,leafboxes,node):\n",
    "    if node.is_a_leaf:\n",
    "        leafboxes.append(Rectangle(node.position,3,1))\n",
    "    else:\n",
    "        nodeboxes.append(Rectangle(node.position,3,1))\n",
    "        add_boxes(nodeboxes,leafboxes,node.left)\n",
    "        add_boxes(nodeboxes,leafboxes,node.right)\n",
    "    return\n",
    "\n",
    "def print_text(node):\n",
    "    if node.is_a_leaf:\n",
    "        plt.text(node.position[0],node.position[1]+0.8, \"value=\"+str(node.leaf_value), size = 35,\\\n",
    "                 family = \"fantasy\", color = \"k\", style = \"italic\", weight = \"light\")\n",
    "    else:\n",
    "        plt.text(node.position[0],node.position[1]+0.8, \"A\"+str(node.attribute)+'>'+str(node.value), size = 35,\\\n",
    "                 family = \"fantasy\", color = \"k\", style = \"italic\", weight = \"light\")\n",
    "        plt.text((node.position[0]+node.left.position[0])/2+1.5,(node.position[1]+node.left.position[1])/2+0.8,'F',size=27,\\\n",
    "                 family = \"fantasy\", color = \"k\", style = \"italic\", weight = \"light\")\n",
    "        plt.text((node.position[0]+node.right.position[0])/2+1.5,(node.position[1]+node.right.position[1])/2+0.8,'T',size=27,\\\n",
    "                 family = \"fantasy\", color = \"k\", style = \"italic\", weight = \"light\")\n",
    "        print_text(node.left)\n",
    "        print_text(node.right)\n",
    "        return\n",
    "\n",
    "def draw_line(node):\n",
    "    if not node.is_a_leaf:\n",
    "        plt.plot([node.position[0]+1.5,node.left.position[0]+1.5],[node.position[1]+1,node.left.position[1]])\n",
    "        plt.plot([node.position[0]+1.5,node.right.position[0]+1.5],[node.position[1]+1,node.right.position[1]])\n",
    "        draw_line(node.left)\n",
    "        draw_line(node.right)\n",
    "        \n",
    "        \n",
    "def make_node_boxes(ax, root,edgecolor='k', alpha=0.3):\n",
    "\n",
    "    # Create list for all the error patches\n",
    "    nodeboxes = []\n",
    "    leafboxes = []\n",
    "\n",
    "    add_boxes(nodeboxes,leafboxes,root)\n",
    "\n",
    "    # Create patch collection with specified colour/alpha\n",
    "    node_pc = PatchCollection(nodeboxes, facecolor='y', alpha=alpha, edgecolor=edgecolor)\n",
    "    leaf_pc = PatchCollection(leafboxes, facecolor='g', alpha=alpha, edgecolor=edgecolor)\n",
    "    # Add collection to axes\n",
    "    ax.add_collection(node_pc)\n",
    "    ax.add_collection(leaf_pc)\n",
    "    print_text(root)\n",
    "    draw_line(root)\n",
    "    return\n",
    "\n",
    "def draw(root,max_width,max_depth,filename):\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(max_width*3.5,max_depth*3+1))\n",
    "    ax.set_xlim(0,max_width*3.5)\n",
    "    ax.set_ylim(max_depth*6+6)\n",
    "    make_node_boxes(ax, root)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "#     plt.show()\n",
    "    return\n",
    "\n",
    "def draw_tree(root,depth,filename):\n",
    "    depth_node_count = defaultdict(lambda: 0)\n",
    "    depth_node_track = defaultdict(lambda: 0)\n",
    "    count_node(root,depth_node_count)\n",
    "    max_width = max(depth_node_count.values())\n",
    "    max_depth = depth\n",
    "    assign_position(root,depth_node_count,depth_node_track,max_width,max_depth)\n",
    "    draw(root,max_width,max_depth,filename)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data,node):\n",
    "    if node.is_a_leaf:\n",
    "        return int(data[-1]),int(node.leaf_value)\n",
    "    else:\n",
    "        if data[node.attribute]>node.value:\n",
    "            predict,actual = forward(data,node.right)\n",
    "        else:\n",
    "            predict,actual = forward(data,node.left)\n",
    "        return predict,actual\n",
    "  \n",
    "def calc_recall(confusion_matrix,label):\n",
    "    if np.sum(confusion_matrix[label]) == 0:\n",
    "        return 0.0\n",
    "    return confusion_matrix[label,label]/np.sum(confusion_matrix[label])\n",
    "\n",
    "\n",
    "def calc_precision(confusion_matrix,label):\n",
    "    if np.sum(confusion_matrix[:,label]) == 0:\n",
    "        return 0.0\n",
    "    return confusion_matrix[label,label]/np.sum(confusion_matrix[:,label])\n",
    "\n",
    "\n",
    "def calc_F1(recall,precision):\n",
    "    if recall == 0 or precision == 0:\n",
    "        return 0.0\n",
    "    return 2/(1/(recall)+1/precision)\n",
    "\n",
    "\n",
    "def evaluate(test_db,trained_tree):\n",
    "    confusion_matrix = np.zeros((4,4))\n",
    "    for data in test_db:\n",
    "        predict,actual = forward(data,trained_tree)\n",
    "        confusion_matrix[actual-1,predict-1] += 1\n",
    "    label_arr = np.zeros((4,3))\n",
    "    for label in range(4):\n",
    "        recall = calc_recall(confusion_matrix,label)\n",
    "        precision = calc_precision(confusion_matrix,label)\n",
    "        F1 = calc_F1(recall,precision)\n",
    "        \n",
    "        label_arr[label,0]=recall\n",
    "        label_arr[label,1]=precision\n",
    "        label_arr[label,2]=F1\n",
    "        \n",
    "    classification_rate = np.sum(confusion_matrix*np.identity(4))/np.sum(confusion_matrix)\n",
    "    return confusion_matrix,label_arr,classification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_tree(node):\n",
    "    if node.is_a_leaf:\n",
    "        return deepcopy(node)\n",
    "    else:\n",
    "        node_copy = deepcopy(node)\n",
    "        node_copy.left = copy_tree(node.left)\n",
    "        node_copy.right = copy_tree(node.right)\n",
    "        return node_copy\n",
    "    \n",
    "    \n",
    "def next_double_leaves_node(node):\n",
    "    if node.is_a_leaf or node.keep:\n",
    "        return None\n",
    "    elif node.left.is_a_leaf and node.right.is_a_leaf and not node.keep:\n",
    "        return node\n",
    "    else:\n",
    "        next_node = next_double_leaves_node(node.left)\n",
    "        if not next_node:\n",
    "            next_node = next_double_leaves_node(node.right)\n",
    "        return next_node\n",
    "    \n",
    "    \n",
    "def prun(root,dataset):\n",
    "    root_copy = copy_tree(root)\n",
    "    _,_,ori_acc = evaluate(dataset,root_copy)\n",
    "    next_node = next_double_leaves_node(root_copy)\n",
    "    while next_node:\n",
    "        next_node.is_a_leaf = True\n",
    "        next_node.leaf_value = next_node.major_value\n",
    "        _,_,acc = evaluate(dataset,root_copy)\n",
    "        if acc<=ori_acc:\n",
    "            next_node.keep = True\n",
    "            next_node.is_a_leaf = False\n",
    "        else:\n",
    "            ori_acc = acc\n",
    "            next_node.left = None\n",
    "            next_node.right = None\n",
    "        next_node = next_double_leaves_node(root_copy)\n",
    "    return root_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dataset_name,n=10,pruning=False,seed=0,draw=True):\n",
    "    print('procissing',dataset_name,',pruning=',str(pruning),':')\n",
    "    result_folder = dataset_name[:-4]+'_result'\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.mkdir(result_folder)\n",
    "    CMs = np.zeros((4,4))\n",
    "    stat = np.zeros((4,3))\n",
    "    average_classification_rate = 0\n",
    "    dataset = np.loadtxt(dataset_name)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    for i in range(n):\n",
    "        train_dataset,test_dataset = split_dataset(dataset,n,i)\n",
    "        if pruning:\n",
    "            for j in range(9):\n",
    "                print(str(i+j/9)[:3]+'/10',end = '\\r',flush = True)\n",
    "                pruning_train_dataset,pruning_validationg_dataset = split_dataset(train_dataset,9,j)\n",
    "                root,depth = decision_tree_learning(pruning_train_dataset,0)\n",
    "                root = prun(root,pruning_validationg_dataset)\n",
    "                img_name = 'plt'+str(i)+'_'+str(j)+'_prun.png'\n",
    "                if draw:\n",
    "                    draw_tree(root,depth,os.path.join(result_folder,img_name))\n",
    "                confusion_matrix,label_arr,classification_rate = evaluate(test_dataset,root)\n",
    "                CMs+=confusion_matrix\n",
    "                stat += label_arr\n",
    "                average_classification_rate += classification_rate\n",
    "        else:\n",
    "            print(str(i)+'/10',end = '\\r',flush = True)\n",
    "            root,depth = decision_tree_learning(train_dataset,0)\n",
    "            img_name = 'plt_'+str(i)+'.png'\n",
    "            if draw:\n",
    "                draw_tree(root,depth,os.path.join(result_folder,img_name))\n",
    "            confusion_matrix,label_arr,classification_rate = evaluate(test_dataset,root)\n",
    "            CMs+=confusion_matrix\n",
    "            stat += label_arr\n",
    "            average_classification_rate += classification_rate\n",
    "    if pruning:\n",
    "        CMs /= 90\n",
    "        stat /= 90\n",
    "        average_classification_rate /= 90\n",
    "    else:\n",
    "        CMs /= n\n",
    "        stat /= n\n",
    "        average_classification_rate /= n\n",
    "    return CMs,stat,average_classification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procissing clean_dataset.txt ,pruning= True :\n",
      "9.8/10\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.seterr(invalid='ignore')\n",
    "#     clean_CMs,clean_stat,clean_ave_class_rate = process(r'clean_dataset.txt',pruning=False,draw=False)\n",
    "    clean_CMs_pruning,clean_stat_pruning,clean_ave_class_rate_pruning = process(r'clean_dataset.txt', pruning=True,draw=False)\n",
    "#     noisy_CMs,noisy_stat,noisy_ave_class_rate = process(r'noisy_dataset.txt',pruning=False)\n",
    "#     noisy_CMs_pruning,noisy_stat_pruning,noisy_ave_class_rate_pruning = process(r'noisy_dataset.txt',pruning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49.47777778  0.          0.48888889  0.43333333]\n",
      " [ 0.         47.71111111  1.98888889  0.        ]\n",
      " [ 0.2         2.28888889 47.22222222  0.3       ]\n",
      " [ 0.32222222  0.          0.3        49.26666667]]\n",
      "[[0.98099168 0.98906177 0.98484996]\n",
      " [0.9602623  0.95456038 0.95702135]\n",
      " [0.9449445  0.9452241  0.94458654]\n",
      " [0.98781454 0.98557969 0.98660366]]\n",
      "0.9683888888888887\n"
     ]
    }
   ],
   "source": [
    "print(clean_CMs_pruning)\n",
    "print(clean_stat_pruning)\n",
    "print(clean_ave_class_rate_pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
